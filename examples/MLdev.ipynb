{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import lineid_plot\n",
    "from ramandecompy import spectrafit\n",
    "from ramandecompy import peakidentify\n",
    "from ramandecompy import dataprep\n",
    "from ramandecompy import datavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('ML_calibration_dataset')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/water.xlsx', 'H2O')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx', 'Hydrogen')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Methane_Baseline_Calibration.xlsx', 'Methane')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/CarbonMonoxide_Baseline_Calibration.xlsx', 'CarbonMonoxide')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/CO2_100wt%.csv', 'CO2')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/sapphire.xlsx', 'sapphire')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Propane_test.xlsx', 'Propane')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Ethane_test.xlsx', 'Ethane')\n",
    "dataprep.add_calibration('ML_calibration_dataset.hdf5', '../ramandecompy/tests/test_files/Acetaldehyde_test.xlsx', 'Acetaldehyde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('ML_calibration_dataset.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'ML_calibration_dataset.hdf5'\n",
    "dataprep.view_hdf5(hdf5_calfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('ML_quad_calibration.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('dataimport_ML_df.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('ML_exp_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('ML_exp_test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filename = 'ML_exp_test.hdf5'\n",
    "dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_35s.csv')\n",
    "dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_45s.csv')\n",
    "dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_55s.csv')\n",
    "dataprep.add_experiment(hdf5_filename, '../ramandecompy/tests/test_files/FA_3.6wt%_300C_65s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'ML_calibration_dataset.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'ML_exp_test.hdf5'\n",
    "key = '300C/25s'\n",
    "df1 = peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'ML_calibration_dataset.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'ML_exp_test.hdf5'\n",
    "key = '300C/35s'\n",
    "df2 = peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'ML_calibration_dataset.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'ML_exp_test.hdf5'\n",
    "key = '300C/45s'\n",
    "df3 = peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'ML_calibration_dataset.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'ML_exp_test.hdf5'\n",
    "key = '300C/55s'\n",
    "df4 = peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'ML_calibration_dataset.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'ML_exp_test.hdf5'\n",
    "key = '300C/65s'\n",
    "df5 = peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1,df2,df3,df4,df5]\n",
    "result = pd.concat(frames,axis=0, join='outer', join_axes= None, ignore_index=False,\n",
    "          keys=['25s','35s','45s','55s','65s'], levels=None,  names=None, verify_integrity=False,\n",
    "          copy=True,sort=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module\n",
    "from sklearn import preprocessing\n",
    "# create the Labelencoder object\n",
    "le = preprocessing.LabelEncoder()\n",
    "#convert the categorical columns into numeric\n",
    "encoded_value = le.fit_transform([\"['Hydrogen']\", \"['Hydrogen', 'sapphire']\", \"['sapphire']\", \"['Unassigned']\", \"['CO2']\", \"['H2O']\", \"['CarbonMonoxide']\"])\n",
    "print(encoded_value)\n",
    "\n",
    "# to give numeric values for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['fraction']= result[:][0]\n",
    "dat['sigma']= result[:][1]\n",
    "dat['amplitude']= result[:][3]\n",
    "dat['fwhm']= result[:][4]\n",
    "dat['height']= result[:][5]\n",
    "dat['auc']= result[:][6]\n",
    "dat['labelencoded']=le.fit_transform(result[:][7])\n",
    "dat['center']= result[:][2]\n",
    "dat['labels']= result[:][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(ncols=7, figsize=(15, 6))\n",
    "plt.subplots_adjust(wspace=.75, hspace=.75) \n",
    "sns.violinplot(data=dat['fraction'], ax=ax[0],showfliers=True) #boxplot with Outliers are removed from the dataset\n",
    "sns.violinplot(data=dat['sigma'],ax=ax[1],showfliers=True) #boxplot with Outliers are removed from the dataset\n",
    "sns.violinplot(data=dat['center'], ax=ax[2],showfliers=True) #boxplot with Outliers are removed from the dataset\n",
    "sns.violinplot(data=dat['amplitude'],ax=ax[3],showfliers=True) #boxplot with Outliers are removed from the dataset\n",
    "sns.violinplot(data=dat['fwhm'], ax=ax[4],showfliers=True) #boxplot with Outliers are removed from the dataset\n",
    "sns.violinplot(data=dat['height'],ax=ax[5],showfliers=True) #boxplot with Outliers are removed from the dataset\n",
    "sns.violinplot(data=dat['auc'],ax=ax[6],showfliers=True) #boxplot with Outliers are removed from the dataset\n",
    "\n",
    "ax[0].set_ylabel('fraction') \n",
    "ax[1].set_ylabel('sigma') \n",
    "ax[2].set_ylabel('center') \n",
    "ax[3].set_ylabel('amp') \n",
    "ax[4].set_ylabel('fwhm') \n",
    "ax[5].set_ylabel('height') \n",
    "ax[6].set_ylabel('auc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'ML_calibration_test.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'ML_calibration_test.hdf5'\n",
    "frames = []\n",
    "for i,key in enumerate(key_list):\n",
    "    df =peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 10, plot =False)\n",
    "    frames.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames,axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module\n",
    "from sklearn import preprocessing\n",
    "# create the Labelencoder object\n",
    "le = preprocessing.LabelEncoder()\n",
    "cal['fraction']= result[:][0]\n",
    "cal['sigma']= result[:][1]\n",
    "# dat['center']= result[:][2]\n",
    "cal['amplitude']= result[:][3]\n",
    "cal['fwhm']= result[:][4]\n",
    "cal['height']= result[:][5]\n",
    "cal['auc']= result[:][6]\n",
    "cal['labelencoded']=le.fit_transform(result[:][7])\n",
    "cal['labels']= result[:][7]\n",
    "cal['center']= result[:][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Not working so far, NEED Help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(5, 3))\n",
    "# y = dat['fraction'].values\n",
    "# x = dat['height'].values\n",
    "# # Create the linear regression model\n",
    "# LogRegr = linear_model.LogisticRegression() \n",
    "# # plotting the balance\n",
    "# ax.scatter(dat['height'], dat['fraction'], label = 'balance default',color='r')\n",
    "\n",
    "# # fit the linear model \n",
    "# LogRegr.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
    "# xres = np.array(x).reshape(-1,1)\n",
    "# plt.plot(x,LogRegr.predict_proba(xres)[:,1],'o',label = 'fit')\n",
    "# # Find the coefficients B0 and B1\n",
    "# print('B0, B1: ',LogRegr.intercept_, LogRegr.coef_[0])\n",
    "# ax.set_ylabel('Probability Default')\n",
    "# ax.set_xlabel('Balance')\n",
    "# ax.set_title('Probability Default vs. Balance')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least squares Regression Model by StatsModels (Not working so far, NEED Help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Ordinary least squares regression model\n",
    "# results = smf.ols(formula = 'auc ~ height*sigma', data = dat)\n",
    "# # Fit the results\n",
    "# resultfit = results.fit()\n",
    "# # Inspect the results\n",
    "# print(resultfit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=['fraction','sigma','amplitude','fwhm','height','auc','labelencoded']\n",
    "X_train=cal[inputs]\n",
    "y_train = cal['center']\n",
    "X_test=dat[inputs]\n",
    "y_test= dat['center']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=1010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNmodel = KNeighborsClassifier(n_neighbors=15) # initialize the model by choosing the number of neighbors (flexibility of model in this case)\n",
    "# KNNmodel = KNeighborsClassifier(n_neighbors=1) minimum neighbord minimum predicted error\n",
    "# remember the parabolic function of training vs. test set there is limit to the flexilibility vs. errors of the model\n",
    "# Expected n_neighbors <= n_samples,  but n_samples = 15, n_neighbors = 20\n",
    "# thus KNNmodel = KNeighborsClassifier(n_neighbors=15) is max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNmodel.fit(X_train, y_train) # feed the model some training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rate = KNNmodel.predict(X_train) == y_train # USE TO PREDICT NEW X DATA PUT INTO THE TRAINING MODEL\n",
    "print('Training Error Rate:', np.mean(rate)) # COMPARING THE PREDICTED VALUES WITH ACTUAL RATE VALUES\n",
    "\n",
    "rate = KNNmodel.predict(X_test) == y_test\n",
    "print('Testing Error Rate:', np.mean(rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate.mean() ratio of trues \n",
    "rate.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knn visualiztion (not working, NEED help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # additional library we will use \n",
    "# from matplotlib.colors import ListedColormap\n",
    "\n",
    "# # just for convenience and similarity with sklearn tutorial\n",
    "# # I am going to assign our X and Y data to specific vectors\n",
    "# # this is not strictly needed and you could use elements df for the whole thing!\n",
    "# elements = dat\n",
    "# X=elements[['height','auc']]\n",
    "# print(X)\n",
    "# #this is a trick to turn our strings (type of element / class) into unique \n",
    "# #numbers.  Play with this in a separate cell and make sure you know wth is \n",
    "# #going on!\n",
    "# levels,labels=pd.factorize(elements.labels)\n",
    "# y=levels\n",
    "# print(levels)\n",
    "# #This determines levelspacing for our color map and the colors themselves\n",
    "# h=0.02\n",
    "# cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "# cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# # in the sklearn tutorial two different weights are compared\n",
    "# # the decision between \"uniform\" and \"distance\" determines the probability\n",
    "# # weight.  \"uniform\" is the version presented in class, you can change to \n",
    "# # distance\n",
    "# # weights='uniform'\n",
    "# weights='distance'\n",
    "\n",
    "# # I am actually refitting the KNN here. If you had a big data set you would\n",
    "# # not do this, but I want you to have the convenience of changing K or \n",
    "# # weights here in this cell. Large training sets with many features can take \n",
    "# # awhile for KNN training! \n",
    "\n",
    "# K=5\n",
    "# clf = KNeighborsClassifier(n_neighbors=K, weights=weights)\n",
    "# clf.fit(X,y)\n",
    "\n",
    "# # Straight from the tutorial - quickly read and see if you know what these \n",
    "# # things are going - if you are < 5 min until end then you should skip this part \n",
    "\n",
    "# # Plot the decision boundary. For that, we will assign a color to each\n",
    "# # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "# x_min, x_max = elements.height.min() - 0.1  , elements.height.max() + 0.1\n",
    "# y_min, y_max = elements.auc.min() - 0.1  , elements.auc.max() + 0.1  \n",
    "# print(y_min, y_max, h)\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \n",
    "#                      np.arange(y_min, y_max, h))\n",
    "\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# # Put the result into a color plot\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.figure(figsize=(4,4));\n",
    "# #plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "# plt.pcolormesh(xx, yy, Z, cmap=cmap_light,vmin=0,vmax=np.max(y))\n",
    "# # Plot also the training points\n",
    "# # This may be the 1st time you have seen how to color points by a 3rd vector\n",
    "# # In this case y ( see c=y in below statement ). This is very useful! \n",
    "# plt.scatter(X.height, X.auc, c=y, cmap=cmap_bold)\n",
    "\n",
    "# # Set limits and lebels \n",
    "# plt.xlim(xx.min(), xx.max())\n",
    "# plt.ylim(yy.min(), yy.max())\n",
    "# plt.xlabel('height')\n",
    "# plt.ylabel('auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(dat, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numdescriptors = 8\n",
    "# train linear model of descriptors\n",
    "yrange =  np.arange(1,numdescriptors,1)\n",
    "for Y in yrange:\n",
    "    MLR=linear_model.LinearRegression()\n",
    "    MLR.fit(train[train.columns.values[0:Y]],train[train.columns.values[Y]])\n",
    "    # WE are going to train using the first 8 values\n",
    "\n",
    "    # make predictions on test and train set \n",
    "    trainpred=MLR.predict(train[train.columns.values[0:Y]])\n",
    "    # predict the outputs using the training dataset\n",
    "    testpred=MLR.predict(test[train.columns.values[0:Y]])\n",
    "    # predict using test dataset\n",
    "    #make parity plot \n",
    "    maxlimit  = train[train.columns.values[Y]].max()\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlim([0,maxlimit]);\n",
    "    plt.ylim([0,maxlimit]);\n",
    "    print('This is for Y ='+str(Y))\n",
    "    plt.scatter(train[train.columns.values[Y]],trainpred, label='Training')\n",
    "    plt.scatter(test[train.columns.values[Y]],testpred,color='r', label='Test')\n",
    "    plt.plot([0,maxlimit],[0,maxlimit],lw=4,color='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Actual Output')\n",
    "    plt.ylabel('Predicted Output')\n",
    "\n",
    "    #calculate the test and train error\n",
    "    \n",
    "    print(\"Train error\",mean_squared_error(train[train.columns.values[Y]],trainpred)) # MSE of training dataset\n",
    "    print(\"Test error\",mean_squared_error(test[train.columns.values[Y]],testpred))\n",
    "    # usually the training error is less than the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,yactual,color='red',lw='3',label='actual')\n",
    "plt.plot(x,regr.predict(x.reshape(-1,1)),ls='--',label='fit')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.scatter(x, yactual, label='actual')\n",
    "ax.scatter(x, yrand, label='measured')\n",
    "ax.scatter(x, regr.predict(x.reshape(-1,1)), label='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.scatter(yrand, regr.predict(x.reshape(-1,1)), label='parity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.scatter(x, regr.predict(x.reshape(-1,1)) - yrand)\n",
    "ax.plot(x, 0*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing begining and ending columns to describe regression relationships\n",
    "# ie changing X and Y\n",
    "# NEEED TO CHECK WHETHER Y=6 means AUC is the comparison column\n",
    "# THis will relate to which descriptor i am trying to relate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numdescriptors = 6 # Y= 6 if AUC must be last Y cannot be 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xrange =  np.arange(0,numdescriptors,1)\n",
    "Y=numdescriptors\n",
    "for X in xrange:    \n",
    "    MLR=linear_model.LinearRegression()\n",
    "    MLR.fit(train[train.columns.values[X:Y]],train[train.columns.values[Y]])\n",
    "    # WE are going to train using the first 8 values\n",
    "\n",
    "    # make predictions on test and train set \n",
    "    trainpred=MLR.predict(train[train.columns.values[X:Y]])\n",
    "    # predict the outputs using the training dataset\n",
    "    testpred=MLR.predict(test[train.columns.values[X:Y]])\n",
    "    # predict using test dataset\n",
    "    #make parity plot \n",
    "    maxlimit  = train[train.columns.values[Y]].max()\n",
    "    print(maxlimit)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlim([0,maxlimit]);\n",
    "    plt.ylim([0,maxlimit]);\n",
    "    print('This is for X ='+str(X)+ '& Y ='+str(Y))\n",
    "    plt.scatter(train[train.columns.values[Y]],trainpred, label='Training')\n",
    "    plt.scatter(test[train.columns.values[Y]],testpred,color='r', label='Test')\n",
    "    plt.plot([0,maxlimit],[0,maxlimit],lw=4,color='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Actual Output')\n",
    "    plt.ylabel('Predicted Output')\n",
    "\n",
    "    #calculate the test and train error\n",
    "    \n",
    "    print(\"Train error\",mean_squared_error(train[train.columns.values[Y]],trainpred)) # MSE of training dataset\n",
    "    print(\"Test error\",mean_squared_error(test[train.columns.values[Y]],testpred))\n",
    "    # usually the training error is less than the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge & Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "#from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "#normalized data for Ridge / LASSO \n",
    "# train_normalized=train/train.std()\n",
    "# test_normalized=test/test.std()\n",
    "scaler = StandardScaler().fit(train[inputs])\n",
    "train_normalized = pd.DataFrame(data=scaler.transform(train[inputs]), columns=train.columns[:-1])\n",
    "test_normalized = pd.DataFrame(data=scaler.transform(test[inputs]), columns=test.columns[:-1])\n",
    "# Columns cannot have strings because could not convert string to float: \"['H2O']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_normalized.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raman_ridge=Ridge()\n",
    "numdescriptors = 7 # Y=6 for AUC to be last\n",
    "yrange =  np.arange(1,numdescriptors,1)\n",
    "for Y in yrange:\n",
    "    a=1e1\n",
    "    raman_ridge.set_params(alpha=a)\n",
    "    raman_ridge.fit(train_normalized[train.columns.values[0:Y]],train_normalized[train.columns.values[Y]])\n",
    "    print('This is for Y ='+str(Y))\n",
    "    print(\"Train error\",mean_squared_error(train_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            train_normalized[train.columns.values[0:Y]])))\n",
    "\n",
    "    print(\"Test error\",mean_squared_error(test_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            test_normalized[train.columns.values[0:Y]])))\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlim([-2.5,2.5]);\n",
    "    plt.ylim([-2.5,2.5]);\n",
    "    plt.scatter(train_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            train_normalized[train.columns.values[0:Y]]), label='Training')\n",
    "    plt.scatter(test_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            test_normalized[train.columns.values[0:Y]]),color='r', label='Test')\n",
    "    maxlimit  = train[train.columns.values[Y]].max()\n",
    "    minlimit  = train[train.columns.values[Y]].min()\n",
    "    plt.plot([-2.5,2.5],[-2.5,2.5],lw=4,color='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Actual Output')\n",
    "    plt.ylabel('Predicted Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RIDGE REGRESSION LOOKING AT RELATIONSHIPS OF ALL Descriptors vs. AUC I think!!\n",
    "raman_ridge=Ridge()\n",
    "numdescriptors = 7\n",
    "yrange =  np.arange(1,numdescriptors,1)\n",
    "for Y in yrange:\n",
    "    a=1e1\n",
    "    raman_ridge.set_params(alpha=a)\n",
    "    raman_ridge.fit(train_normalized[train.columns.values[0:Y]],\n",
    "                    train_normalized[train.columns.values[Y]])\n",
    "    print('This is for Y ='+str(Y))\n",
    "    print(\"Train error\",mean_squared_error(train_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            train_normalized[train.columns.values[0:Y]])))\n",
    "\n",
    "    print(\"Test error\",mean_squared_error(test_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            test_normalized[train.columns.values[0:Y]])))\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlim([-2.5,2.5]);\n",
    "    plt.ylim([-2.5,2.5]);\n",
    "    plt.scatter(train_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            train_normalized[train.columns.values[0:Y]]), label='Training')\n",
    "    plt.scatter(test_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            test_normalized[train.columns.values[0:Y]]),color='r', label='Test')\n",
    "    maxlimit  = train[train.columns.values[Y]].max()\n",
    "    minlimit  = train[train.columns.values[Y]].min()\n",
    "    plt.plot([-2.5,2.5],[-2.5,2.5],lw=4,color='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Actual Output')\n",
    "    plt.ylabel('Predicted Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raman_ridge=Ridge()\n",
    "numdescriptors = 6 # Y=6 for AUC to be last\n",
    "xrange =  np.arange(0,numdescriptors,1)\n",
    "Y=numdescriptors\n",
    "for X in xrange:\n",
    "    a=1e1\n",
    "    raman_ridge.set_params(alpha=a)\n",
    "    raman_ridge.fit(train_normalized[train.columns.values[X:Y]],train_normalized[train.columns.values[Y]])\n",
    "    print('This is for X ='+str(X)+'& Y =' + str(Y))\n",
    "    print(\"Train error\",mean_squared_error(train_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            train_normalized[train.columns.values[X:Y]])))\n",
    "\n",
    "    print(\"Test error\",mean_squared_error(test_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            test_normalized[train.columns.values[X:Y]])))\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlim([-2.5,2.5]);\n",
    "    plt.ylim([-2.5,2.5]);\n",
    "    plt.scatter(train_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            train_normalized[train.columns.values[X:Y]]), label='Training')\n",
    "    plt.scatter(test_normalized[train.columns.values[Y]],raman_ridge.predict(\n",
    "            test_normalized[train.columns.values[X:Y]]),color='r', label='Test')\n",
    "    maxlimit  = train[train.columns.values[Y]].max()\n",
    "    minlimit  = train[train.columns.values[Y]].min()\n",
    "    plt.plot([-2.5,2.5],[-2.5,2.5],lw=4,color='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Actual Output')\n",
    "    plt.ylabel('Predicted Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Example of searching the $\\alpha$ space in RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(dat, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR vs lambda (based on sklearn tutorial)\n",
    "coefs = []\n",
    "trainerror = []\n",
    "testerror = []\n",
    "\n",
    "Y = numdescriptors\n",
    "X = 0\n",
    "# do you know what is happening here? \n",
    "lambdas = np.logspace(-6,6,200) # SEACH THE RANGE OF THE LAMBDA SPACE\n",
    "model=Ridge()\n",
    "\n",
    "# loop over lambda values (strength of regularization)\n",
    "for l in lambdas:\n",
    "    model.set_params(alpha=l)\n",
    "    model.fit(train_normalized[train.columns.values[X:Y]],train_normalized[train.columns.values[Y]])\n",
    "    coefs.append(model.coef_)\n",
    "    trainerror.append(mean_squared_error(train_normalized[train.columns.values[Y]],model.predict(\n",
    "        train_normalized[train.columns.values[X:Y]])))\n",
    "    testerror.append(mean_squared_error(test_normalized[train.columns.values[Y]],model.predict(\n",
    "        test_normalized[train.columns.values[X:Y]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is being plotted here? \n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(lambdas,coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('coefs')\n",
    "plt.title('RR coefs vs $\\lambda$')\n",
    "plt.subplot(122)\n",
    "plt.plot(lambdas,trainerror,label='train error')\n",
    "plt.plot(lambdas,testerror,label='test error')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc=1)\n",
    "plt.title('error vs $\\lambda$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: LASSO regression  (same data as Part 1)\n",
    "\n",
    "* The lasso improves over ridge by also providing a variable selection tool!\n",
    "* The lasso minimizer is $RSS + \\lambda \\sum_{j=1}^{p}\\lvert\\beta_j\\rvert$\n",
    "* Ridge regression does not set any of the coefficients exactly to zero but can shrink all of them\n",
    "- final model still includes all p predictors\n",
    "- Lasso is similar to ridge regression with a absolute value\n",
    "- It provides the possibility that some of the coeeficients can take a value of zero.\n",
    "- Like ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# also based on sklearn tutorials\n",
    "# what the hell is happening in this cell?\n",
    "coefs = []\n",
    "trainerror = []\n",
    "testerror = []\n",
    "\n",
    "lambdas = np.logspace(-6,6,200)\n",
    "model=linear_model.Lasso()\n",
    "numdescriptors = 6 # Y must be 6 for AUC to be last\n",
    "Y = numdescriptors\n",
    "X = 0\n",
    "# loop over lambda values (strength of regularization)\n",
    "for l in lambdas:\n",
    "    model.set_params(alpha=l,max_iter=1e6)\n",
    "    model.fit(train_normalized[train.columns.values[X:Y]],train_normalized[train.columns.values[Y]])\n",
    "    coefs.append(model.coef_)\n",
    "    trainerror.append(mean_squared_error(train_normalized[train.columns.values[Y]],model.predict(\n",
    "        train_normalized[train.columns.values[X:Y]])))\n",
    "    testerror.append(mean_squared_error(test_normalized[train.columns.values[Y]],model.predict(\n",
    "        test_normalized[train.columns.values[X:Y]])))\n",
    "yrange =  np.arange(1,7,1)\n",
    "for Y in yrange:\n",
    "    a=1e1\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(train_normalized[train.columns.values[X:Y]],train_normalized[train.columns.values[Y]])\n",
    "    print('This is for Y ='+str(Y))\n",
    "    print(\"Train error\",trainerror[Y])\n",
    "    print(\"Test error\",testerror[Y])\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlim([-2.5,2.5]);\n",
    "    plt.ylim([-2.5,2.5]);\n",
    "    plt.scatter(train_normalized[train.columns.values[Y]],model.predict(\n",
    "        train_normalized[train.columns.values[X:Y]]), label='Training')\n",
    "    plt.scatter(test_normalized[train.columns.values[Y]],model.predict(\n",
    "        test_normalized[train.columns.values[X:Y]]),color='r', label='Test')\n",
    "    maxlimit  = train[train.columns.values[Y]].max()\n",
    "    minlimit  = train[train.columns.values[Y]].min()\n",
    "    plt.plot([-2.5,2.5],[-2.5,2.5],lw=4,color='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Actual Output')\n",
    "    plt.ylabel('Predicted Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# also based on sklearn tutorials\n",
    "# what the hell is happening in this cell?\n",
    "coefs = []\n",
    "trainerror = []\n",
    "testerror = []\n",
    "\n",
    "lambdas = np.logspace(-6,6,200)\n",
    "model=linear_model.Lasso()\n",
    "numdescriptors = 6 # Y must be 6 for AUC to be last\n",
    "Y = numdescriptors\n",
    "X = 0\n",
    "# loop over lambda values (strength of regularization)\n",
    "for l in lambdas:\n",
    "    model.set_params(alpha=l,max_iter=1e6)\n",
    "    model.fit(train_normalized[train.columns.values[X:Y]],train_normalized[train.columns.values[Y]])\n",
    "    coefs.append(model.coef_)\n",
    "    trainerror.append(mean_squared_error(train_normalized[train.columns.values[Y]],model.predict(\n",
    "        train_normalized[train.columns.values[X:Y]])))\n",
    "    testerror.append(mean_squared_error(test_normalized[train.columns.values[Y]],model.predict(\n",
    "        test_normalized[train.columns.values[X:Y]])))\n",
    "xrange =  np.arange(0,numdescriptors-1,1)\n",
    "for X in xrange:\n",
    "    a=1e1\n",
    "    model.set_params(alpha=a)\n",
    "    model.fit(train_normalized[train.columns.values[X:Y]],train_normalized[train.columns.values[Y]])\n",
    "    print('This is for Y ='+str(Y))\n",
    "    print(\"Train error\",trainerror[Y])\n",
    "    print(\"Test error\",testerror[Y])\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xlim([-2.5,2.5]);\n",
    "    plt.ylim([-2.5,2.5]);\n",
    "    plt.scatter(train_normalized[train.columns.values[Y]],model.predict(\n",
    "        train_normalized[train.columns.values[X:Y]]), label='Training')\n",
    "    plt.scatter(test_normalized[train.columns.values[Y]],model.predict(\n",
    "        test_normalized[train.columns.values[X:Y]]),color='r', label='Test')\n",
    "    maxlimit  = train[train.columns.values[Y]].max()\n",
    "    minlimit  = train[train.columns.values[Y]].min()\n",
    "    plt.plot([-2.5,2.5],[-2.5,2.5],lw=4,color='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Actual Output')\n",
    "    plt.ylabel('Predicted Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.locator_params(nbins=5)\n",
    "plt.subplot(121)\n",
    "plt.plot(lambdas,coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('coefs')\n",
    "plt.title('RR coefs vs $\\lambda$')\n",
    "#plt.xlim(1e-4,1e0)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(lambdas,trainerror,label='train error')\n",
    "plt.plot(lambdas,testerror,label='test error')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "#plt.xlim(1e-4,1e0)\n",
    "#plt.ylim(0,0.5)\n",
    "plt.legend(loc=1)\n",
    "plt.title('error vs $\\lambda$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE SIMPLIFIED OUR MODEL BY GETTING RID OF THE RR COEFS NOT EQUAL TO ZERO AT ZERO LAMBDA.\n",
    "\n",
    "\n",
    "### Other things to consider if you have more time \n",
    "\n",
    "* Note we did not scale the features in the MLR, try it out and verify the final error doesnt' change!\n",
    "* Make sure you undersand how to make _predictions_ with supervised learning models that are trained on scaled/normalized data\n",
    "* Plot the residuals and verify if errors are distributed normally\n",
    "* Make a parity plot including the predictions from ridge and LASSO \n",
    "* Compare errors between all three \n",
    "* Explore the effect of training/testing split \n",
    "* Look at the shrinkage/regularization situation when predicting Y2 vs Y1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to see if any of these descriptors \n",
    "# ['fraction','sigma','center','amplitude','fwhm','height','auc']\n",
    "\n",
    "# can relate to the peakidentified labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=['fraction','sigma','center','amplitude','fwhm','height','auc']\n",
    "X=dat[inputs]\n",
    "y = dat['labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model Precision: what percentage of positive tuples are labeled as such?\n",
    "# print(\"Precision:\",metrics.precision_score(y_test, y_pred),average='macro')\n",
    "\n",
    "# # Model Recall: what percentage of positive tuples are labelled as such?\n",
    "# print(\"Recall:\",metrics.recall_score(y_test, y_pred), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial kernal (https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='poly', degree=8)  \n",
    "svclassifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='rbf')  \n",
    "svclassifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='sigmoid')  \n",
    "svclassifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees # specifically to see if below descriptors can find and predict Area under curve # Strings needed to be converted to floats anyways so the labelling relationship was not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT=tree.DecisionTreeRegressor(max_depth=3)\n",
    "DT.fit(train[['fraction','sigma','center','amplitude','fwhm','height']],train.auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#conda install -c conda-forge pydotplus=2.0.2\n",
    "# http://www.webgraphviz.com\n",
    "import pydotplus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"basic_tree.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(DT, out_file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.webgraphviz.com\n",
    "# os.unlink('basic_tree.dot')\n",
    "\n",
    "dot_data = tree.export_graphviz(DT, out_file=None,feature_names=['fraction','sigma','center','amplitude','fwhm','height'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "#graph.write_pdf(\"basic_tree.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Users/koolk/Anaconda3/Library/bin/graphviz/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "display(SVG(graph.create_svg()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test and train set \n",
    "trainpred=DT.predict(train[['fraction','sigma','center','amplitude','fwhm','height']])\n",
    "testpred=DT.predict(test[['fraction','sigma','center','amplitude','fwhm','height']])\n",
    "\n",
    "maxlimit =testpred.max()\n",
    "print(maxlimit)\n",
    "#parity plot \n",
    "plt.figure(figsize=(4,4))\n",
    "plt.xlim([0,maxlimit]);\n",
    "plt.ylim([0,maxlimit]);\n",
    "plt.scatter(train.auc,trainpred, label ='train')\n",
    "plt.scatter(test.auc,testpred,color='r', label = 'test')\n",
    "plt.plot([0,maxlimit],[0,maxlimit],lw=4,color='black')\n",
    "plt.legend()\n",
    "#calculate the test and train error\n",
    "print(\"Train error\",mean_squared_error(train.auc,trainpred))\n",
    "print(\"Test error\",mean_squared_error(test.auc,testpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainerror = []\n",
    "testerror = []\n",
    "\n",
    "trees = np.arange(1,20,1)\n",
    "#model=tree.DecisionTreeRegressor()\n",
    "\n",
    "# loop over lambda values (strength of regularization)\n",
    "for t in trees:\n",
    "    model=tree.DecisionTreeRegressor(max_depth=t)\n",
    "    model.fit(train[['fraction','sigma','center','amplitude','fwhm','height']],train.auc)\n",
    "    trainerror.append(mean_squared_error(train.auc,model.predict(\n",
    "        train[['fraction','sigma','center','amplitude','fwhm','height']])))\n",
    "    testerror.append(mean_squared_error(test.auc,model.predict(\n",
    "        test[['fraction','sigma','center','amplitude','fwhm','height']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(trees,trainerror,marker='o',label='testerror')\n",
    "plt.plot(trees,testerror,marker=\"s\",label='trainerror')\n",
    "plt.legend()\n",
    "plt.xlabel('Max tree depth')\n",
    "plt.ylabel('MSE for $auc$')\n",
    "plt.subplot(122)\n",
    "plt.plot(trees,trainerror,marker='o',label='testerror')\n",
    "plt.plot(trees,testerror,marker=\"s\",label='trainerror')\n",
    "plt.ylim((0,1))\n",
    "plt.xlim((5,15))\n",
    "plt.legend()\n",
    "plt.xlabel('Max tree depth')\n",
    "plt.ylabel('MSE for $auc$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM from lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1, class_sep=0.7)\n",
    "rng = np.random.RandomState(2)\n",
    "#X += 2 * rng.uniform(size=X.shape)\n",
    "data = (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "           edgecolors='k', label='Test')\n",
    "#ax.set_xlim([-3,4])\n",
    "#ax.set_ylim([0,4])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=1)\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "Zbi = Z > np.median(Z)\n",
    "ax.contourf(xx, yy, Zbi, cmap=cm, alpha=.1)\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "           edgecolors='k', alpha=0.6, label='Test')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1, class_sep=0.8)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "data = (X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "           edgecolors='k', label='Test')\n",
    "#ax.set_xlim([-3,4])\n",
    "#ax.set_ylim([0,4])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.00000001, 0.001, 0.1, 1, 10, 100, 1000, 10000]\n",
    "figure, axes = plt.subplots(nrows=2, ncols=4, figsize=(12, 9))\n",
    "\n",
    "axes = [item for sublist in axes for item in sublist]\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "for C, ax in zip(C_values, axes):\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    Zbi = Z > np.median(Z)\n",
    "    ax.contourf(xx, yy, Zbi, cmap=cm, alpha=.1)\n",
    "\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k', label='Training')\n",
    "    # Plot the testing points\n",
    "#     ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "#                edgecolors='k', alpha=0.6, label='Test')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title('C Value: {}'.format(str(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "data = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "X, y = data\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=1)\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "Zbi = Z > np.median(Z)\n",
    "ax.contourf(xx, yy, Zbi, cmap=cm, alpha=.1)\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "           edgecolors='k', alpha=0.6, label='Test')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma=4, C=0.1)\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "Zbi = Z > np.median(Z)\n",
    "ax.contourf(xx, yy, Zbi, cmap=cm, alpha=.1)\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "           edgecolors='k', alpha=0.6, label='Test')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation from NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "seed = 21899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['mass', 'voc', 'jsc', 'e_homo_alpha', 'e_gap_alpha', \n",
    "        'e_lumo_alpha']].values\n",
    "Y = df[['pce']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pn, X_test_pn, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the scaler from the training data only and keep it for later use\n",
    "X_train_scaler = StandardScaler().fit(X_train_pn)\n",
    "# apply the scaler transform to the training data\n",
    "X_train = X_train_scaler.transform(X_train_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_train_scaler.transform(X_test_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = estimator.model.evaluate(X_test, y_test)\n",
    "print(\"test set mse is %.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (-1 * results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=simple_model,\n",
    "        epochs=150, batch_size=25000, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
    "print('MSE mean: %.4f ; std: %.4f' % (-1 * results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1000)\n",
    "y = 0.2*x + 1*x**2 - 0.3*x**3 + 0.021*x**4 + 0.3*np.random.rand(1000)*x\n",
    "\n",
    "plt.scatter(x, y, alpha=0.1)\n",
    "\n",
    "X = pd.DataFrame({'x':x, 'x2':x**2, 'x3':x**3, 'x4':x**4, 'x5':x**5, 'x6':x**6, 'x7':x**7, 'x8':x**8,\n",
    "                  'x9':x**9, 'x10':x**10, 'x11':x**12, 'x13':x**13, 'x14':x**14, 'x15':x**15, 'x16':x**16, 'x17':x**17})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train[['x']], y_train) #Simple LINEAR REGRESSION with multiple polynomial \n",
    "#PUT IN ARRAYS OF ONE DATASET OR DATASET OF INDEPENDENT FEATURES\n",
    "y_pred = reg.predict(X_test[['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, alpha=0.1)\n",
    "plt.plot(X_test[['x']], y_pred, linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(x, y, alpha=0.1)\n",
    "mse = np.zeros(17)\n",
    "\n",
    "for i in range(1, 17):\n",
    "    reg = LinearRegression().fit(X_train[X_train.columns[0:i].values], y_train)\n",
    "    y_pred = reg.predict(X_test[X_test.columns[0:i].values])\n",
    "\n",
    "    ax.scatter(X_test[['x']], y_pred, marker='o', s=25, label=i)\n",
    "    mse[i-1] = mean_squared_error(y_test, y_pred)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(np.linspace(1, 16, 17), mse, linewidth=4)\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.set_xlabel('Degree of Polynomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "for run in range(8):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    mse = np.zeros(17)\n",
    "\n",
    "    for i in range(1, 18):\n",
    "        reg = LinearRegression().fit(X_train[X_train.columns[0:i].values], y_train)\n",
    "        y_pred = reg.predict(X_test[X_test.columns[0:i].values])\n",
    "\n",
    "        mse[i-1] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(1, 16, 17), mse, linewidth=4, color='b', alpha=0.2)\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xlabel('Degree of Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf.split(X,y) # now is an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "mse = np.zeros((17, 10))\n",
    "j = 0\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    for i in range(1, 18):\n",
    "        reg = LinearRegression().fit(X_train[X_train.columns[0:i].values], y_train)\n",
    "        y_pred = reg.predict(X_test[X_test.columns[0:i].values])\n",
    "\n",
    "        mse[i-1, j] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(1, 16, 17), mse[:, j], linewidth=4, color='b', alpha=0.09)\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xlabel('Degree of Polynomial')\n",
    "    j += 1\n",
    "    \n",
    "avg_mse = mse.mean(axis=1)\n",
    "ax.plot(np.linspace(1, 16, 17), avg_mse, color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../examples/quad.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = estimator.model.evaluate(X_test, y_test)\n",
    "print(\"test set mse is %.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit_result[i][0] = p[i]_fraction\n",
    "# fit_result[i][1] = p[i]_simga\n",
    "# fit_result[i][2] = p[i]_center\n",
    "# fit_result[i][3] = p[i]_amplitude\n",
    "# fit_result[i][4] = p[i]_fwhm\n",
    "# fit_result[i][5] = p[i]_height\n",
    "# fit_result[i][6] = p[i]_area under the curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
