{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raman Spectroscopy Decomposition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Once components in a mixture Raman spectra have been identified and assigned, and psudo-Voigt curve fiting has been completed the next step is to compare pure component calibration (or non-decomposing, non-reacting) area under peaks to experimental data. From this comparision one will be able to deterimine:\n",
    "1. is decomposition occuring?\n",
    "\n",
    "and if it is then: \n",
    "2. calculate the amount of molar decomposition \n",
    "\n",
    "This calculation can be completed by comparing the area value of the experimental mixture Raman spectra to the pure component calibration Raman spectra area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-step (1/2): Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ramandecompy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9ef9b741c1a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mramandecompy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectrafit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mramandecompy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpeakidentify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mramandecompy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataprep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ramandecompy'"
     ]
    }
   ],
   "source": [
    "#initial imports\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from ramandecompy import spectrafit\n",
    "from ramandecompy import peakidentify\n",
    "from ramandecompy import dataprep\n",
    "from ramandecompy import dataimport\n",
    "from ramandecompy import datavis\n",
    "from ramandecompy import machine_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-step (2/2): Import Calibration / Pure Component Raman Spectra Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('calibration_data2')\n",
    "\n",
    "dataprep.add_calibration('calibration_data2.hdf5',\n",
    "                          '../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "                          label='Hydrogen')\n",
    "dataprep.add_calibration('calibration_data2.hdf5',\n",
    "                         '../ramandecompy/tests/test_files/CarbonMonoxide_Baseline_Calibration.xlsx',\n",
    "                         label='CarbonMonoxide')\n",
    "dataprep.add_calibration('calibration_data2.hdf5','../ramandecompy/tests/test_files/CO2_100wt%.csv',label='CO2')\n",
    "dataprep.add_calibration('calibration_data2.hdf5','../ramandecompy/tests/test_files/water.xlsx',label='H2O')\n",
    "dataprep.add_calibration('calibration_data2.hdf5','../ramandecompy/tests/test_files/sapphire.xlsx',label='sapphire')\n",
    "dataprep.add_calibration('calibration_data2.hdf5','../ramandecompy/tests/test_files/FormicAcid_3_6percent.xlsx',label='FormicAcid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('calibration_data2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('calibration_data2-Copy1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('dataimport_ML_df-Copy1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Experimental Data Sets\n",
    "The first thing is to put experimental data into a hdf5 file (this file will end up being used to identify peaks)\n",
    "\n",
    "With multiple files in a directory/ many data sets it is usefull to loop over all files in the directory to add versus adding one by one. The code to loop came from a stackoverflow comment: `https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory`\n",
    "\n",
    "Note: A good resource for HDF5 file types in general is: `http://docs.h5py.org/en/stable/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataprep.new_hdf5('dataprep_experiment') #comment this line out once made for the first time so an error isn't given saying that the file already exists\n",
    "#directory = '/Users/elizabeth/Desktop/raman-spectra-decomp-analysis/ramandecompy/tests/test_files/' #defining directory for data\n",
    "#dataprep.view_hdf5('dataprep_experimental.hdf5')\n",
    "\n",
    "#base_dir = '../ramandecompy/tests/test_files/'\n",
    "\n",
    "#for filename in os.listdir(directory):\n",
    "#    if filename.startswith('FA_') and filename.endswith('.csv'):\n",
    "#        locationandfile = directory + filename\n",
    "#        dataprep.add_experiment('dataprep_experimental.hdf5', locationandfile)\n",
    "#        continue\n",
    "#    else:\n",
    "#        continue\n",
    "#return\n",
    "\n",
    "#FOR CALIBRATION DATA MASS ADD\n",
    "\n",
    "#dataprep.new_hdf5('dataprep_experiment') #comment this line out once made for the first time so an error isn't given saying that the file already exists\n",
    "#directory = '/Users/elizabeth/Desktop/raman-spectra-decomp-analysis/ramandecompy/tests/test_files/' #defining directory for data\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#type(filename) #checking the type (making sure is a string) for file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dataprep.view_hdf5('dataimport_ML_df-Copy1.hdf5') #making sure the loop did its job and all data is correctly imported \n",
    "#comment out this to not see the long list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define substance of interest\n",
    "The second step is to determine if the desired speciecies in the spectra is present, and if it is then if it has decomposed (decreased/changed) from the defined calibration area. \n",
    "\n",
    "At this point this will be done by the user knowing where the approximate location of the peak for the substance that is of interest. \n",
    "\n",
    "Given the user center peak wavelength location input the code will go through the calibration data and for a peak with a center at the defined location (ith some tolerance of +/- 10 cm^-1) will take the area of that curve and store it as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting peak information for Formic Acid \n",
    "\n",
    "data1 = h5py.File('calibration_data2.hdf5', 'r+')\n",
    "# then specify the peak\n",
    "peak_01 = list(data1['FormicAcid/Peak_01'][0])\n",
    "#peak_01s = data1['FormicAcid/Peak_01']\n",
    "# you put list because otherwise it just saves it as a h5py.dataset or something and lists are more familiar. Then peak_01 will be a list containing the 7 elements of the Peak_01 dataset\n",
    "print(peak_01)\n",
    "\n",
    "type(peak_01)\n",
    "#print(type(peak_01s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_01[6] #Looking for area under the curve value for the first peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out what peak from Formic Acid to use/ are close to other observed peaks used in analysis\n",
    "For Formic Acid, prior reports of the wavenumbers of significant Raman Peaks (cm^-1) were at:\n",
    "- 712\n",
    "- 1219\n",
    "- 1400\n",
    "- 1714\n",
    "- 2943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_01 = list(data1['FormicAcid/Peak_01'][0])\n",
    "print(peak_01[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_02 = list(data1['FormicAcid/Peak_02'][0])\n",
    "print(peak_02[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_03 = list(data1['FormicAcid/Peak_03'][0])\n",
    "print(peak_03[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_04 = list(data1['FormicAcid/Peak_04'][0])\n",
    "print(peak_04[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_05 = list(data1['FormicAcid/Peak_05'][0])\n",
    "print(peak_05[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_06 = list(data1['FormicAcid/Peak_06'][0])\n",
    "print(peak_06[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All peaks previously reported are identified in the calibration file within +/- 5 wavenumbers\n",
    "There is an additional peak identified at 1055, but it is hypothesized that this peak may not be easily identified from other components with similar wavenumbers and/or with the amplitude of the peak being smaller then peaks of formic acid at the other wavenumbers this could be why it isn't identifed in literature. \n",
    "\n",
    "For this example the peak occuring at 1400 cm^-1 (peak_04) will be used for molar concentration calculations **because... NEED TO FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_cal_area = peak_04[6]\n",
    "print(FA_cal_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define presence of substance in experimental data\n",
    "\n",
    "Then for that same center peak wavelength location input it will identify the presence of the peak (if it is there) in the experimental data and area for that peak and store it as a second variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define filenames\n",
    "hdf5_calfilename = 'calibration_data2.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'dataimport_ML_df-Copy1.hdf5'\n",
    "\n",
    "cal_key_list = machine_learning.keyfinder(hdf5_calfilename)\n",
    "exp_key_list = machine_learning.keyfinder(hdf5_expfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list= machine_learning.keyfinder('dataimport_ML_df-Copy1.hdf5')\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'calibration_data2.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'dataimport_ML_df-Copy2.hdf5'\n",
    "\n",
    "df = peakidentify.peak_assignment(hdf5_expfilename, '300C/25s', hdf5_calfilename, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_calfilename = 'calibration_data2.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'dataimport_ML_df-Copy1.hdf5'\n",
    "\n",
    "frames = []\n",
    "for i, key in enumerate(key_list):\n",
    "    df = peakidentify.peak_assignment(hdf5_expfilename, '300C/25s', hdf5_calfilename, 10)\n",
    "    frames.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames,axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "         keys=None, levels=None, names=None, verify_integrity=False,\n",
    "         copy=True,sort=True)\n",
    "\n",
    "\n",
    "dat = pd.DataFrame()\n",
    "\n",
    "\n",
    "dat['fraction']= result[:][0]\n",
    "dat['sigma']= result[:][1]\n",
    "# dat['center']= result[:][2]\n",
    "dat['amplitude']= result[:][3]\n",
    "dat['fwhm']= result[:][4]\n",
    "dat['height']= result[:][5]\n",
    "dat['auc']= result[:][6]\n",
    "dat['labelencoded']=le.fit_transform(result[:][7])\n",
    "dat['labels']= result[:][7]\n",
    "dat['center']= result[:][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key_list in 'dataimport_ML_df-Copy1.hdf5':\n",
    "    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Step 5: Calculate Molar Decomposition\n",
    "\n",
    "To define the molar decomposition the area of the experimental data will be divided by the calibration data's area. This value will be the molar amount of the substance at the given experimental temperature and resonance time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Plot Molar Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Molar Decomposition with Reported Literature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
