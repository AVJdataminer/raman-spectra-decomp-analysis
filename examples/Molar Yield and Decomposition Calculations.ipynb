{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raman Spectroscopy Decomposition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Once components in a mixture Raman spectra have been identified and assigned, and psudo-Voigt curve fiting has been completed the next step is to compare pure component calibration (or non-decomposing, non-reacting) area under peaks to experimental data. From this comparision one will be able to deterimine:\n",
    "1. is decomposition occuring?\n",
    "\n",
    "and if it is then: \n",
    "2. calculate the amount of molar decomposition \n",
    "\n",
    "This calculation can be completed by comparing the area value of the experimental mixture Raman spectra to the pure component calibration Raman spectra area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-step (1/2): Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dataimport' from 'ramandecompy' (/anaconda3/lib/python3.7/site-packages/ramandecompy-1.0b0-py3.7.egg/ramandecompy/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-211a44c60e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mramandecompy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpeakidentify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mramandecompy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataprep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mramandecompy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataimport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mramandecompy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatavis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'dataimport' from 'ramandecompy' (/anaconda3/lib/python3.7/site-packages/ramandecompy-1.0b0-py3.7.egg/ramandecompy/__init__.py)"
     ]
    }
   ],
   "source": [
    "#initial imports\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import lineid_plot\n",
    "from ramandecompy import spectrafit\n",
    "from ramandecompy import peakidentify\n",
    "from ramandecompy import dataprep\n",
    "from ramandecompy import dataimport\n",
    "from ramandecompy import datavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-step (2/2): Import Calibration / Pure Component Raman Spectra Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.new_hdf5('calibration_data1')\n",
    "\n",
    "dataprep.add_calibration('calibration_data1.hdf5',\n",
    "                          '../ramandecompy/tests/test_files/Hydrogen_Baseline_Calibration.xlsx',\n",
    "                          label='Hydrogen')\n",
    "dataprep.add_calibration('calibration_data1.hdf5',\n",
    "                          '../ramandecompy/tests/test_files/CarbonMonoxide_Baseline_Calibration.xlsx',\n",
    "                          label='CarbonMonoxide')\n",
    "dataprep.add_calibration('calibration_data1.hdf5','../ramandecompy/tests/test_files/CO2_100wt%.csv',label='CO2')\n",
    "dataprep.add_calibration('calibration_data1.hdf5','../ramandecompy/tests/test_files/water.xlsx',label='H2O')\n",
    "dataprep.add_calibration('calibration_data1.hdf5','../ramandecompy/tests/test_files/sapphire.xlsx',label='sapphire')\n",
    "dataprep.add_calibration('calibration_data1.hdf5','../ramandecompy/tests/test_files/FormicAcid_3_6percent.xlsx',label='FormicAcid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** calibration_data1.hdf5 ****\n",
      "\u001b[1mCO2\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    counts\n",
      "|    wavenumber\n",
      "\u001b[1mCarbonMonoxide\u001b[0m\n",
      "|    Peak_01\n",
      "|    counts\n",
      "|    wavenumber\n",
      "\u001b[1mFormicAcid\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    Peak_05\n",
      "|    Peak_06\n",
      "|    counts\n",
      "|    wavenumber\n",
      "\u001b[1mH2O\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    counts\n",
      "|    wavenumber\n",
      "\u001b[1mHydrogen\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    counts\n",
      "|    wavenumber\n",
      "\u001b[1msapphire\u001b[0m\n",
      "|    Peak_01\n",
      "|    Peak_02\n",
      "|    Peak_03\n",
      "|    Peak_04\n",
      "|    counts\n",
      "|    wavenumber\n"
     ]
    }
   ],
   "source": [
    "dataprep.view_hdf5('calibration_data1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All peaks previously reported are identified in the calibration file within +/- 5 wavenumbers\n",
    "There is an additional peak identified at 1055, but it is hypothesized that this peak may not be easily identified from other components with similar wavenumbers and/or with the amplitude of the peak being smaller then peaks of formic acid at the other wavenumbers this could be why it isn't identifed in literature. \n",
    "\n",
    "For this example the peak occuring at 1400 cm^-1 (peak_04) will be used for molar concentration calculations **because... NEED TO FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17613.377535487394\n"
     ]
    }
   ],
   "source": [
    "FA_cal_area = peak_04[6]\n",
    "print(FA_cal_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Experimental Data Sets\n",
    "The first thing is to put experimental data into a hdf5 file (this file will end up being used to identify peaks)\n",
    "\n",
    "With multiple files in a directory/ many data sets it is usefull to loop over all files in the directory to add versus adding one by one. The code to loop came from a stackoverflow comment: `https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory`\n",
    "\n",
    "Note: A good resource for HDF5 file types in general is: `http://docs.h5py.org/en/stable/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataprep.new_hdf5('dataprep_experiment') #comment this line out once made for the first time so an error isn't given saying that the file already exists\n",
    "#directory = '/Users/elizabeth/Desktop/raman-spectra-decomp-analysis/ramandecompy/tests/test_files/' #defining directory for data\n",
    "#dataprep.view_hdf5('dataprep_experimental.hdf5')\n",
    "\n",
    "#base_dir = '../ramandecompy/tests/test_files/'\n",
    "\n",
    "#for filename in os.listdir(directory):\n",
    "#    if filename.startswith('FA_') and filename.endswith('.csv'):\n",
    "#        locationandfile = directory + filename\n",
    "#        dataprep.add_experiment('dataprep_experimental.hdf5', locationandfile)\n",
    "#        continue\n",
    "#    else:\n",
    "#        continue\n",
    "#return\n",
    "\n",
    "#FOR CALIBRATION DATA MASS ADD\n",
    "\n",
    "#dataprep.new_hdf5('dataprep_experiment') #comment this line out once made for the first time so an error isn't given saying that the file already exists\n",
    "#directory = '/Users/elizabeth/Desktop/raman-spectra-decomp-analysis/ramandecompy/tests/test_files/' #defining directory for data\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#type(filename) #checking the type (making sure is a string) for file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dataprep.view_hdf5('dataimport_ML_df-Copy1.hdf5') #making sure the loop did its job and all data is correctly imported \n",
    "#comment out this to not see the long list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define substance of interest\n",
    "The second step is to determine if the desired speciecies in the spectra is present, and if it is then if it has decomposed (decreased/changed) from the defined calibration area. \n",
    "\n",
    "At this point this will be done by the user knowing where the approximate location of the peak for the substance that is of interest. \n",
    "\n",
    "Given the user center peak wavelength location input the code will go through the calibration data and for a peak with a center at the defined location (ith some tolerance of +/- 10 cm^-1) will take the area of that curve and store it as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.810769612166254, 19.283679639361118, 707.31, 12300.420389104662, 38.567359278722236, 221.31494220141718, 12142.523207238122]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting peak information for Formic Acid \n",
    "\n",
    "data1 = h5py.File('calibration_data1.hdf5', 'r')\n",
    "# then specify the peak\n",
    "peak_01 = list(data1['FormicAcid/Peak_01'])\n",
    "# you put list because otherwise it just saves it as a h5py.dataset or something and lists are more familiar. Then peak_01 will be a list containing the 7 elements of the Peak_01 dataset\n",
    "print(peak_01)\n",
    "\n",
    "type(peak_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12142.523207238122"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_01[6] #Looking for area under the curve value for the first peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out what peak from Formic Acid to use/ are close to other observed peaks used in analysis\n",
    "For Formic Acid, prior reports of the wavenumbers of significant Raman Peaks (cm^-1) were at:\n",
    "- 712\n",
    "- 1219\n",
    "- 1400\n",
    "- 1714\n",
    "- 2943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707.31\n"
     ]
    }
   ],
   "source": [
    "peak_01 = list(data1['FormicAcid/Peak_01'])\n",
    "print(peak_01[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055.9\n"
     ]
    }
   ],
   "source": [
    "peak_02 = list(data1['FormicAcid/Peak_02'])\n",
    "print(peak_02[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219.5\n"
     ]
    }
   ],
   "source": [
    "peak_03 = list(data1['FormicAcid/Peak_03'])\n",
    "print(peak_03[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400.1\n"
     ]
    }
   ],
   "source": [
    "peak_04 = list(data1['FormicAcid/Peak_04'])\n",
    "print(peak_04[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1716.7\n"
     ]
    }
   ],
   "source": [
    "peak_05 = list(data1['FormicAcid/Peak_05'])\n",
    "print(peak_05[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2940.6\n"
     ]
    }
   ],
   "source": [
    "peak_06 = list(data1['FormicAcid/Peak_06'])\n",
    "print(peak_06[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All peaks previously reported are identified in the calibration file within +/- 5 wavenumbers\n",
    "There is an additional peak identified at 1055, but it is hypothesized that this peak may not be easily identified from other components with similar wavenumbers and/or with the amplitude of the peak being smaller then peaks of formic acid at the other wavenumbers this could be why it isn't identifed in literature. \n",
    "\n",
    "For this example the peak occuring at 1400 cm^-1 (peak_04) will be used for molar concentration calculations **because... NEED TO FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17613.377535487394\n"
     ]
    }
   ],
   "source": [
    "FA_cal_area = peak_04[6]\n",
    "print(FA_cal_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define presence of substance in experimental data\n",
    "\n",
    "Then for that same center peak wavelength location input it will identify the presence of the peak (if it is there) in the experimental data and area for that peak and store it as a second variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyfinder function\n",
    "def keyfinder(hdf5_filename):\n",
    "   seconds = []\n",
    "   hdf5 = h5py.File(hdf5_filename, 'r')\n",
    "   for _, layer_1 in enumerate(list(hdf5.keys())):\n",
    "       if isinstance(hdf5[layer_1], h5py.Group):\n",
    "   #         print('\\033[1m{}\\033[0m'.format(layer_1))\n",
    "           for _, layer_2 in enumerate(list(hdf5[layer_1].keys())):\n",
    "               if isinstance(hdf5['{}/{}'.format(layer_1, layer_2)], h5py.Group):\n",
    "   #                 print('|    \\033[1m{}\\033[0m'.format(layer_2))\n",
    "                   seconds.append('{}/{}'.format(layer_1, layer_2))\n",
    "                   for _, layer_3 in enumerate(list(hdf5['{}/{}'.format(layer_1, layer_2)])):\n",
    "                       if isinstance(hdf5['{}/{}/{}'.format(layer_1, layer_2, layer_3)],\n",
    "                                     h5py.Group):\n",
    "   #                         print('|    |    \\033[1m{}\\033[0m/...'.format(layer_3))\n",
    "                           pass\n",
    "                       else:\n",
    "                           pass\n",
    "   #                         print('|    |    {}'.format(layer_3))\n",
    "               else:\n",
    "   #                 print('|    {}'.format(layer_2))\n",
    "                   seconds.append('{}/{}'.format(layer_1, layer_2))\n",
    "       else:\n",
    "           pass\n",
    "   #         print('{}'.format(layer_1))\n",
    "   hdf5.close()\n",
    "   return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Passed value of `known_compound_list` is not a list! Instead, it is: <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-47c70afb87b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mexpkey\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'300C/25s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mpeakidentify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeak_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_expfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_calfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_list\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-47c70afb87b7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mexpkey\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'300C/25s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mpeakidentify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeak_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_expfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_calfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_list\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/ramandecompy-1.0b0-py3.7.egg/ramandecompy/peakidentify.py\u001b[0m in \u001b[0;36mpeak_assignment\u001b[0;34m(unknownhdf5_filename, unknownkey, knownhdf5_filename, known_compound_list, precision, plot)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown_compound_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         raise TypeError(\"Passed value of `known_compound_list` is not a list! Instead, it is: \"\n\u001b[0;32m---> 23\u001b[0;31m                         + str(type(known_compound_list)))\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# handling input errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknownhdf5_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Passed value of `known_compound_list` is not a list! Instead, it is: <class 'int'>"
     ]
    }
   ],
   "source": [
    "hdf5_calfilename = 'calibration_data1.hdf5' #update to hdf5_calfilename\n",
    "hdf5_expfilename = 'dataimport_ML_df-Copy1.hdf5'\n",
    "\n",
    "cal_key_list = keyfinder(hdf5_calfilename)\n",
    "exp_key_list = keyfinder(hdf5_expfilename)\n",
    "\n",
    "expkey ='300C/25s'\n",
    "\n",
    "frames = [ peakidentify.peak_assignment(hdf5_expfilename, key, hdf5_calfilename, 50) for key in key_list ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-0376148c0c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m result = pd.concat(frames,axis=0, join='outer', join_axes=None, ignore_index=False,\n\u001b[1;32m      2\u001b[0m           \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           copy=True,sort=True)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "result = pd.concat(frames,axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=key_list, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Step 5: Calculate Molar Decomposition\n",
    "\n",
    "To define the molar decomposition the area of the experimental data will be divided by the calibration data's area. This value will be the molar amount of the substance at the given experimental temperature and resonance time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Plot Molar Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Molar Decomposition with Reported Literature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
