{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataimport.py example\n",
    "\n",
    "This notebook will show how the functions contained within the `dataimport.py` module are used to generate hdf5 files for storing raw data and peak fitting results. This module is also the primary way in which the functions contained within spectrafit.py are utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This model allows a user who has all their experimental data saved in a directory folder to mass import the files and have an hdf5 file created that has the data organized and fit. This module interacts closely with the dataprep.py module also\n",
    "# included with this package. \n",
    "\n",
    "# The advantage of this model is that it is an automated (looped) version of the `add_experiment` function in dataprep.py thus allowing a user not to have to manually import data files one at a time.\n",
    "\n",
    "# The user needs to have had an organized filename structure as the organized hdf5 file relies on it. For this use case the compound name 'FA_' (Formic Acid) is the first part of the files in the directory and then the \n",
    "\n",
    "# Developed by the Raman-Noodles team (2019 DIRECT Cohort, University of Washington)\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "#initial imports\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from ramandecompy import dataprep\n",
    "\n",
    "\n",
    "def data_import(hdf5_filename, directory):\n",
    "#     \"\"\"\n",
    "#     This function adds Raman experimental data to an existing hdf5 file. It uses the\n",
    "#     spectrafit.fit_data function to fit the data before saving the fit result and\n",
    "#     the raw data to the hdf5 file. The data_filename must be in a standardized format\n",
    "#     to interact properly with this function. It must take the form anyname_temp_time.xlsx\n",
    "#     (or .csv) since this function will parse the the temp and time from the filename to\n",
    "#     label the data and fit result in the hdf5 file.\n",
    "\n",
    "#     Args:\n",
    "#         hdf5_filename (str): the filename and location of an existing hdf5 file to add the\n",
    "#                              experiment data too. Variable must be in a string format.\n",
    "\n",
    "\n",
    "\n",
    "#         directory (str): the folder location of raw Raman spectroscopy data in a \n",
    "#                              string format.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "    # open hdf5 file as read/write\n",
    "    dataprep.new_hdf5(hdf5_filename)\n",
    "    exp_file = h5py.File(hdf5_filename+'.hdf5', 'r+')\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('FA_') and filename.endswith('.csv'):\n",
    "            locationandfile = directory + filename\n",
    "            dataprep.add_experiment(str(hdf5_filename)+'.hdf5', locationandfile)\n",
    "            print('Data from {} fit with compound pseudo-Voigt model. Results saved to {}.'.format(filename, hdf5_filename))\n",
    "            # printing out to user the status of the import (because it can take a long time if importing a lot of data,\n",
    "            # about minute/data set for test files\n",
    "            exp_file.close()\n",
    "            continue\n",
    "        else:\n",
    "            print('Data from {} fit with compound pseudo-Voigt model. Results saved to {}.'.format(filename, hdf5_filename))\n",
    "            exp_file.close()\n",
    "            continue\n",
    "    return\n",
    "\n",
    "def add_experiment(hdf5_filename, exp_filename):\n",
    "    \"\"\"\n",
    "    This function adds Raman experimental data to an existing hdf5 file. It uses the\n",
    "    spectrafit.fit_data function to fit the data before saving the fit result and\n",
    "    the raw data to the hdf5 file. The data_filename must be in a standardized format\n",
    "    to interact properly with this function. It must take the form anyname_temp_time.xlsx\n",
    "    (or .csv) since this function will parse the the temp and time from the filename to\n",
    "    label the data and fit result in the hdf5 file.\n",
    "\n",
    "    Args:\n",
    "        hdf5_filename (str): the filename and location of an existing hdf5 file to add the\n",
    "                             experiment data too.\n",
    "        exp_filename (str): the filename and location of raw Raman spectroscopy data in\n",
    "                             either the form of an .xlsx or a .csv with the wavenumber data\n",
    "                             in the 1st column and the counts data in the 2nd column. These\n",
    "                             files should contain only the wavenumber and counts data\n",
    "                             (no column labels).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # handling input errors\n",
    "    if not isinstance(hdf5_filename, str):\n",
    "        raise TypeError('Passed value of `hdf5_filename` is not a string! Instead, it is: '\n",
    "                        + str(type(hdf5_filename)))\n",
    "    if not hdf5_filename.split('/')[-1].split('.')[-1] == 'hdf5':\n",
    "        raise TypeError('`hdf5_filename` is not type = .hdf5! Instead, it is: '\n",
    "                        + hdf5_filename.split('/')[-1].split('.')[-1])\n",
    "    if not isinstance(exp_filename, str):\n",
    "        raise TypeError('Passed value of `data_filename` is not a string! Instead, it is: '\n",
    "                        + str(type(exp_filename)))\n",
    "    # confirm exp_filename is correct format (can handle additional decimals in exp_filename\n",
    "    label = '.'.join(exp_filename.split('/')[-1].split('.')[:-1])\n",
    "    if len(label.split('_')) < 2:\n",
    "        raise ValueError(\"\"\"Passed value of `exp_filename` inapproprate. exp_filename must contain\n",
    "        at least one '_', preferably of the format somename_temp_time.xlsx (or .csv)\"\"\")\n",
    "    # r+ is read/write mode and will fail if the file does not exist\n",
    "    exp_file = h5py.File(hdf5_filename, 'r+')\n",
    "    if exp_filename.split('.')[-1] == 'xlsx':\n",
    "        data = pd.read_excel(exp_filename, header=None, names=('wavenumber', 'counts'))\n",
    "    elif exp_filename.split('.')[-1] == 'csv':\n",
    "        data = pd.read_csv(exp_filename, header=None, names=('wavenumber', 'counts'))\n",
    "    else:\n",
    "        print('data file type not recognized')\n",
    "    # ensure that the data is listed from smallest wavenumber first\n",
    "    if data['wavenumber'][:1].values > data['wavenumber'][-1:].values:\n",
    "        data = data.iloc[::-1]\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        pass\n",
    "    # peak detection and data fitting\n",
    "    fit_result, residuals = spectrafit.fit_data(data['wavenumber'].values, data['counts'].values)\n",
    "    # extract experimental parameters from filename\n",
    "    specs = exp_filename.split('/')[-1].split('.')[-2]\n",
    "    if len(specs) > 1:\n",
    "        spec = ''\n",
    "        for _, element in enumerate(specs):\n",
    "            spec = str(spec+element)\n",
    "        specs = spec\n",
    "    specs = specs.split('_')\n",
    "    time = specs[-1]\n",
    "    temp = specs[-2]\n",
    "    # write data to .hdf5\n",
    "    exp_file['{}/{}/wavenumber'.format(temp, time)] = data['wavenumber']\n",
    "    exp_file['{}/{}/counts'.format(temp, time)] = data['counts']\n",
    "    exp_file['{}/{}/residuals'.format(temp, time)] = residuals\n",
    "    for i, result in enumerate(fit_result):\n",
    "        # create custom datatype\n",
    "        my_datatype = np.dtype([('fraction', np.float),\n",
    "                                ('center', np.float),\n",
    "                                ('sigma', np.float),\n",
    "                                ('amplitude', np.float),\n",
    "                                ('fwhm', np.float),\n",
    "                                ('height', np.float),\n",
    "                                ('area under the curve', np.float)])\n",
    "        if i < 9:\n",
    "            dataset = exp_file.create_dataset('{}/{}/Peak_0{}'.format(temp, time, i+1),\n",
    "                                              (1,), dtype=my_datatype)\n",
    "        else:\n",
    "            dataset = exp_file.create_dataset('{}/{}/Peak_{}'.format(temp, time, i+1),\n",
    "                                              (1,), dtype=my_datatype)\n",
    "        # apply data to tuple\n",
    "        data = tuple(result[:7])\n",
    "        data_array = np.array(data, dtype=my_datatype)\n",
    "        # write new values to the blank dataset\n",
    "        dataset[...] = data_array\n",
    "    print(\"\"\"Data from {} fit with compound pseudo-Voigt model.\n",
    "     Results saved to {}.\"\"\".format(exp_filename, hdf5_filename))\n",
    "    exp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import lineid_plot\n",
    "from ramandecompy import spectrafit\n",
    "from ramandecompy import peakidentify\n",
    "from ramandecompy import dataprep\n",
    "from ramandecompy import datavis\n",
    "from ramandecompy import dataimport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The old way\n",
    "### dataprep.add_experiment\n",
    "\n",
    "First we will see how the function `dataprep.add_experiment` operates and how it stores experimental data under groups that specify the temperature and residence time for each experiment added. First we will make a new `experiment.hdf5` file to store the experimental data. Importing this file will take longer than the earlier examples since this spectra contains a larger number of peaks that need to be fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv fit with compound pseudo-Voigt model.\n",
      "     Results saved to olddataprep_experiment.hdf5.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dd9e005b8e9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moldexphdf5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf5_filename\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.hdf5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0madd_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'olddataprep_experiment.hdf5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0madd_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'olddataprep_experiment.hdf5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../ramandecompy/tests/test_files/FA_3.6wt%_400C_12.5s.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# dataprep.add_experiment('olddataprep_experiment.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_300C_45s.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-daef2f448260>\u001b[0m in \u001b[0;36madd_experiment\u001b[1;34m(hdf5_filename, exp_filename)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;31m# write data to .hdf5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mexp_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'{}/{}/wavenumber'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wavenumber'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dataprep.new_hdf5('olddataprep_experiment')\n",
    "hdf5_filename = 'olddataprep_experiment'\n",
    "oldexphdf5 = h5py.File(hdf5_filename+'.hdf5', 'r+')\n",
    "add_experiment('olddataprep_experiment.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_300C_25s.csv')\n",
    "add_experiment('olddataprep_experiment.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_400C_12.5s.csv')\n",
    "# dataprep.add_experiment('olddataprep_experiment.hdf5', '../ramandecompy/tests/test_files/FA_3.6wt%_300C_45s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.view_hdf5('olddataprep_experiment.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldexphdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('olddataprep_experiment.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### dataimport.data_import\n",
    "\n",
    "Next we will see how the slighly different function `dataimport.data_import` operates and how it stores experimental data under groups that specify the temperature and residence time for each experiment added. The function `dataimport.data_import` is essentially a wrapper function which uses the `dataprep.add_experiment` to add all the excel data files \n",
    "\n",
    "First we will make a new `dataimport_experiment.hdf5` file to store the experimental data. Then we will search through the directory `../ramandecompy/tests/test_files/` to \n",
    "\n",
    "Importing this file will take longer than the earlier examples since this spectra contains a larger number of peaks that need to be fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdf5_filename = 'dataimport_experiment'\n",
    "directory = '../ramandecompy/tests/test_files/'\n",
    "# open hdf5 file as read/write\n",
    "data_import(hdf5_filename,directory)\n",
    "dataprep.view_hdf5(hdf5_filename+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open .hdf5\n",
    "hdf5_filename = 'dataimport_experiment'\n",
    "exphdf5 = h5py.File(hdf5_filename+'.hdf5', 'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataprep.view_hdf5(hdf5_filename+'.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the file so that there are no errors - this is done in the basic .py file\n",
    "\n",
    "In order to keep the file system clean, and to avoid errors associated with running this notebook multiple times, we lastly will delete the two .hdf5 files generated by this notebook. Comment out the final cell if you wish you explore these files further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the hdf5 file first to stop all processes using the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exphdf5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('dataimport_experiment.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
