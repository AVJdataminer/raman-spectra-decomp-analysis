{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ramandecompy import spectrafit\n",
    "from ramandecompy import dataprep\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import interpolate\n",
    "from lmfit.models import PseudoVoigtModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = 'ramandecompy/tests/test_files/dataprep_experiment.hdf5'\n",
    "key = '300C/25s'\n",
    "hdf5 = h5py.File(hdf5_file, 'r')\n",
    "x_data = list(hdf5['{}/{}'.format(key, 'wavenumber')])\n",
    "y_data = list(hdf5['{}/{}'.format(key, 'counts')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del hdf5['300C/25s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = dataprep.plot_fit(hdf5_file, key)\n",
    "plt.axvline(x=1270, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "develop a new function that will refit the data using the original fit plus a dictionary of custom locations. The dictionary elements with either be integer wavenumbers, or string Peak_#s, followed by how many peaks should be applied to that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract peak center and height locations from hdf5\n",
    "peaks = []\n",
    "for _,peak in enumerate(list(hdf5[key])[:-2]):\n",
    "    peaks.append((list(hdf5['{}/{}'.format(key, peak)])[2], list(hdf5['{}/{}'.format(key, peak)])[5]))\n",
    "peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "# specify a peaks to add manually\n",
    "peak_loc = [1350, 1385]\n",
    "# interpolate data\n",
    "comp_int = interpolate.interp1d(x_data, y_data, kind='cubic')\n",
    "# iterate through peak_loc\n",
    "peaks_add = []\n",
    "for _,guess in enumerate(peak_loc):\n",
    "    height = comp_int(int(guess))\n",
    "    peaks_add.append((int(guess), int(height)))\n",
    "peaks_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfit.models import PseudoVoigtModel\n",
    "\n",
    "mod, pars = spectrafit.set_params(peaks)\n",
    "peak_list = []\n",
    "for i, _ in enumerate(peaks_add):\n",
    "    prefix = 'p{}_'.format(i+1+len(peaks))\n",
    "    peak = PseudoVoigtModel(prefix=prefix)\n",
    "    pars.update(peak.make_params())\n",
    "    pars[prefix+'center'].set(peaks_add[i][0], vary=True, min=(peaks_add[i][0]-10), max=(peaks_add[i][0]+10))\n",
    "    pars[prefix+'height'].set(min=0.1*peaks_add[i][1])\n",
    "    pars[prefix+'sigma'].set(100, min=1, max=150)\n",
    "    pars[prefix+'amplitude'].set(min=0)\n",
    "    peak_list.append(peak)\n",
    "    mod = mod + peak_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = spectrafit.model_fit(x_data, y_data, mod, pars, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_data = np.asarray(x_data)\n",
    "y_data = np.asarray(y_data)\n",
    "\n",
    "spectrafit.plot_fit(x_data, y_data, out, plot_components=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to explore a bit with loosening the sensitivity of the automatic functions to detect too many peaks and then remove those with too small of an area to contribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peaks = spectrafit.peak_detect(x_data, y_data, height=(0.02*max(y_data)), prominence=(0.01*max(y_data)))[0]\n",
    "# len(peaks)\n",
    "# extract peak center and height locations from hdf5\n",
    "peaks = []\n",
    "for _,peak in enumerate(list(hdf5[key])[:-2]):\n",
    "    peaks.append((list(hdf5['{}/{}'.format(key, peak)])[2], list(hdf5['{}/{}'.format(key, peak)])[5]))\n",
    "peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, pars = spectrafit.set_params(peaks)\n",
    "out = spectrafit.model_fit(x_data, y_data, mod, pars)\n",
    "spectrafit.plot_fit(x_data, y_data, out, plot_components=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep.add_experiment(hdf5_file, 'ramandecompy/tests/test_files/FA_3.6wt__300C_25s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def adjust_peaks(hdf5_file, key, add_list, drop_list=None, plot_fit=False):\n",
    "    # open hdf5_file\n",
    "    hdf5 = h5py.File(hdf5_file, 'r+')\n",
    "    # extract raw x-y data\n",
    "    x_data = np.asarray(hdf5['{}/{}'.format(key, 'wavenumber')])\n",
    "    y_data = np.asarray(hdf5['{}/{}'.format(key, 'counts')])\n",
    "    # extract peak center and height locations from hdf5\n",
    "    peaks = []\n",
    "    for _,peak in enumerate(list(hdf5[key])[:-2]):\n",
    "        peaks.append((list(hdf5['{}/{}'.format(key, peak)])[2], list(hdf5['{}/{}'.format(key, peak)])[5]))\n",
    "    # drop desired tuples from peaks\n",
    "    if drop_list is not None:\n",
    "        drop_index = []\n",
    "        for _,name in enumerate(drop_list):\n",
    "            drop_index.append(int(name.split('_')[-1])-1)\n",
    "        for i,index in enumerate(drop_index):\n",
    "            peaks.pop(index-i)      \n",
    "    else:\n",
    "        pass\n",
    "    # interpolate data\n",
    "    comp_int = interpolate.interp1d(x_data, y_data, kind='cubic')\n",
    "    # iterate through add_list\n",
    "    peaks_add = []\n",
    "    for _,guess in enumerate(add_list):\n",
    "        height = comp_int(int(guess))\n",
    "        peaks_add.append((int(guess), int(height)))\n",
    "    # add new list of peaks to model\n",
    "    mod, pars = spectrafit.set_params(peaks)\n",
    "    peak_list = []\n",
    "    for i, _ in enumerate(peaks_add):\n",
    "        prefix = 'p{}_'.format(i+1+len(peaks))\n",
    "        peak = PseudoVoigtModel(prefix=prefix)\n",
    "        pars.update(peak.make_params())\n",
    "        pars[prefix+'center'].set(peaks_add[i][0], vary=True, min=(peaks_add[i][0]-10), max=(peaks_add[i][0]+10))\n",
    "        pars[prefix+'height'].set(min=0.1*peaks_add[i][1])\n",
    "        pars[prefix+'sigma'].set(100, min=1, max=150)\n",
    "        pars[prefix+'amplitude'].set(min=0)\n",
    "        peak_list.append(peak)\n",
    "        mod = mod + peak_list[i] \n",
    "    # run the fit\n",
    "    out = spectrafit.model_fit(x_data, y_data, mod, pars)\n",
    "    # plot_fit option\n",
    "    if plot_fit is True:\n",
    "        spectrafit.plot_fit(x_data, y_data, out, plot_components=True)\n",
    "    else:\n",
    "        pass\n",
    "    # save fit data\n",
    "    fit_result = spectrafit.export_fit_data(x_data, out)\n",
    "    # sort peaks by center location for saving\n",
    "    fit_result = sorted(fit_result, key=lambda x: int(x[2]))\n",
    "    # delete old fit data\n",
    "    del hdf5['300C/25s']\n",
    "    # write data to .hdf5\n",
    "    hdf5['{}/wavenumber'.format(key)] = x_data\n",
    "    hdf5['{}/counts'.format(key)] = y_data\n",
    "    for i, _ in enumerate(fit_result):\n",
    "        if i < 9:\n",
    "            hdf5['{}/Peak_0{}'.format(key, i+1)] = fit_result[i]\n",
    "        else:\n",
    "            hdf5['{}/Peak_{}'.format(key, i+1)] = fit_result[i]\n",
    "    hdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_list = [1270, 1350, 1385]\n",
    "# add_list = [1270]\n",
    "drop_list = ['Peak_01']\n",
    "hdf5_file = 'ramandecompy/tests/test_files/dataprep_experiment.hdf5'\n",
    "key = '300C/25s'\n",
    "\n",
    "adjust_peaks(hdf5_file, key, add_list, drop_list, plot_fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = 'ramandecompy/tests/test_files/dataprep_experiment.hdf5'\n",
    "dataprep.view_hdf5(hdf5_file)\n",
    "fig, ax = dataprep.plot_fit(hdf5_file, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
